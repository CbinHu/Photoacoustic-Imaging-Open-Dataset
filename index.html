
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Med-VLM-Bench</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            margin: 2em;
            background: #f9f9f9;
            color: #333;
        }
        h1, h2, h3 {
            color: #0056b3;
        }
        pre {
            background: #eee;
            padding: 1em;
            overflow-x: auto;
        }
        img {
            max-width: 100%;
            height: auto;
        }
    </style>
</head>
<body>
    <p>
<h1>🧠 Med-VLM-Bench: A Curated Benchmark Repository for Medical Vision-Language Models</h1></p><p>📚 A comprehensive summary of recent benchmarks for evaluating and training Medical Vision-Language Models (Med-VLMs)</p><p>---</p><p><h2>👨‍💻 Contributors</h2>
- 🧑‍🔬 <strong>Zanting Ye</strong>  
  Southern Medical University  
  📧 yzt2861252880@gmail.com</p><p>- 🧑‍🔬 <strong>Xu Han</strong>  
  Shanghai Jiao Tong University  
  📧 hanxv8826@gmail.com</p><p>---</p><p><h2>📊 GitHub Stats</h2>
<img alt="Stars" src="https://img.shields.io/github/stars/yezanting/Med-VLM-Bench-Summary?style=social" />
<img alt="Forks" src="https://img.shields.io/github/forks/yezanting/Med-VLM-Bench-Summary?style=social" />
<img alt="License" src="https://img.shields.io/github/license/yezanting/Med-VLM-Bench-Summary" />
<img alt="Last Commit" src="https://img.shields.io/github/last-commit/yezanting/Med-VLM-Bench-Summary" /></p><p>---</p><p><h2>🔍 Project Overview</h2>
With the continuous advancement of research on Medical Vision-Language Models (Med-VLMs) and their reasoning capabilities, a number of high-quality, publicly available datasets focusing on medical reasoning have been released between March and May 2025. These datasets provide a solid foundation for the development of multimodal medical AI systems.</p><p><strong>Med-VLM-Bench</strong> is a curated, continuously updated repository of the latest and most important datasets for training and evaluating medical LLMs and VLMs. This project focuses on:</p><p>- ✅ Reasoning-centric multimodal benchmarks
- 📅 Latest datasets published in Mar–May 2025
- 🧠 Foundational datasets from 2023–2024
- 🔗 Direct access to dataset links or HuggingFace/GitHub repositories</p><p>💡 Our knowledge is limited to public sources. We welcome community contributions — feel free to open an issue to share new datasets, and we will update promptly. </p><p>📌Note: The annotation time of the dataset is based on the publication time of the corresponding article.</p><p>---</p><p><h2>⭐ Star 趋势图 (Star History)</h2>
<img alt="Star History Chart" src="https://api.star-history.com/svg?repos=yezanting/Med-VLM-Bench-Summary&amp;type=Date" />
</p>
</body>
</html>
