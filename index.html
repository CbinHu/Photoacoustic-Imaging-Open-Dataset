
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Med-VLM-Bench</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            margin: 2em;
            background: #f9f9f9;
            color: #333;
        }
        h1, h2, h3 {
            color: #0056b3;
        }
        pre {
            background: #eee;
            padding: 1em;
            overflow-x: auto;
        }
        img {
            max-width: 100%;
            height: auto;
        }
    </style>
</head>
<body>
    <p>
<h1>ğŸ§  Med-VLM-Bench: A Curated Benchmark Repository for Medical Vision-Language Models</h1></p><p>ğŸ“š A comprehensive summary of recent benchmarks for evaluating and training Medical Vision-Language Models (Med-VLMs)</p><p>---</p><p><h2>ğŸ‘¨â€ğŸ’» Contributors</h2>
- ğŸ§‘â€ğŸ”¬ <strong>Zanting Ye</strong>  
  Southern Medical University  
  ğŸ“§ yzt2861252880@gmail.com</p><p>- ğŸ§‘â€ğŸ”¬ <strong>Xu Han</strong>  
  Shanghai Jiao Tong University  
  ğŸ“§ hanxv8826@gmail.com</p><p>---</p><p><h2>ğŸ“Š GitHub Stats</h2>
<img alt="Stars" src="https://img.shields.io/github/stars/yezanting/Med-VLM-Bench-Summary?style=social" />
<img alt="Forks" src="https://img.shields.io/github/forks/yezanting/Med-VLM-Bench-Summary?style=social" />
<img alt="License" src="https://img.shields.io/github/license/yezanting/Med-VLM-Bench-Summary" />
<img alt="Last Commit" src="https://img.shields.io/github/last-commit/yezanting/Med-VLM-Bench-Summary" /></p><p>---</p><p><h2>ğŸ” Project Overview</h2>
With the continuous advancement of research on Medical Vision-Language Models (Med-VLMs) and their reasoning capabilities, a number of high-quality, publicly available datasets focusing on medical reasoning have been released between March and May 2025. These datasets provide a solid foundation for the development of multimodal medical AI systems.</p><p><strong>Med-VLM-Bench</strong> is a curated, continuously updated repository of the latest and most important datasets for training and evaluating medical LLMs and VLMs. This project focuses on:</p><p>- âœ… Reasoning-centric multimodal benchmarks
- ğŸ“… Latest datasets published in Marâ€“May 2025
- ğŸ§  Foundational datasets from 2023â€“2024
- ğŸ”— Direct access to dataset links or HuggingFace/GitHub repositories</p><p>ğŸ’¡ Our knowledge is limited to public sources. We welcome community contributions â€” feel free to open an issue to share new datasets, and we will update promptly. </p><p>ğŸ“ŒNote: The annotation time of the dataset is based on the publication time of the corresponding article.</p><p>---</p><p><h2>â­ Star è¶‹åŠ¿å›¾ (Star History)</h2>
<img alt="Star History Chart" src="https://api.star-history.com/svg?repos=yezanting/Med-VLM-Bench-Summary&amp;type=Date" />
</p>
</body>
</html>
