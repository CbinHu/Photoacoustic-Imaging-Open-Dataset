# ðŸ§  Med-VLM-Bench: A Curated Benchmark Repository for Medical Vision-Language Models

ðŸ“š A comprehensive summary of recent benchmarks for evaluating and training Medical Vision-Language Models (Med-VLMs)

---

## ðŸ‘¨â€ðŸ’» Contributors
- ðŸ§‘â€ðŸ”¬ **Zanting Ye**  
  Southern Medical University  
  ðŸ“§ yzt2861252880@gmail.com

- ðŸ§‘â€ðŸ”¬ **Xu Han**  
  Shanghai Jiao Tong University  
  ðŸ“§ hanxv8826@gmail.com

- ðŸ§‘â€ðŸ”¬ **Xiaolong Niu**  
  Southern Medical University

- ðŸ§‘â€ðŸ”¬ **Zian Wang**  
  Shanghai Jiao Tong University

- ðŸ§‘â€ðŸ”¬ **Shengyuan Liu**  
  The Chinese University of Hong Kong  
  ðŸ“§ liushengyuan@link.cuhk.edu.hk

- ðŸ‘¨â€ðŸ« **Lijun Lu**  
  Southern Medical University

---

## ðŸ“Š GitHub Stats
![Stars](https://img.shields.io/github/stars/yezanting/Med-VLM-Bench-Summary?style=social)
![Forks](https://img.shields.io/github/forks/yezanting/Med-VLM-Bench-Summary?style=social)
![License](https://img.shields.io/github/license/yezanting/Med-VLM-Bench-Summary)
![Last Commit](https://img.shields.io/github/last-commit/yezanting/Med-VLM-Bench-Summary)

---

## ðŸ” Project Overview
With the continuous advancement of research on Medical Vision-Language Models (Med-VLMs) and their reasoning capabilities, a number of high-quality, publicly available datasets focusing on medical reasoning have been released between March and May 2025. These datasets provide a solid foundation for the development of multimodal medical AI systems.

**Med-VLM-Bench** is a curated, continuously updated repository of the latest and most important datasets for training and evaluating medical LLMs and VLMs. This project focuses on:

- âœ… Reasoning-centric multimodal benchmarks  
- ðŸ“… Latest datasets published in Marâ€“May 2025  
- ðŸ§  Foundational datasets from 2023â€“2024  
- ðŸ”— Direct access to dataset links or HuggingFace/GitHub repositories  

ðŸ’¡ Our knowledge is limited to public sources. We welcome community contributions â€” feel free to open an issue to share new datasets, and we will update promptly. 

ðŸ“ŒNote: The annotation time of the dataset is based on the publication time of the corresponding article.

---

## ðŸ“¢ News

### ðŸŒŸ Latest Updates
- **2025-06-29**: ðŸŽ‰ Added new datasets/benchmarks **AbdomenAtlas 3.0 (ICCV2025)**, **Derm1M(ICCV2025)**, **MedTVT-R1**, **GEMeX(ICCV2025)** and **HIE-Reasoning(ICML2025)**. Check it out for detailed information and download links!
- **2025-06-18**: ðŸŽ‰ Added new datasets/benchmarks **Lingshu**, **ReasonMed**. Check it out for detailed information and download links!
- **2025-06-11**: ðŸŽ‰ Added some recent datasets and benchmarks!
- **2025-06-11**: ðŸŽ‰ Create our GitHub project!

---

## ðŸ“Š Dataset Summary Table

| Dataset Name | Paper Title | Year / Venue | Data Modality | Task Type | Size | Download Link |
|--------------|-------------|--------------|---------------|-----------|------|---------------|
| **MedTVT-QA** | [MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis](https://arxiv.org/pdf/2506.18512) | 2025.06.23 | Text + Time Series (ECG) + Image (CXR) + Tabular (Lab Test) | Multimodal Medical Reasoning, Multi-disease Diagnosis, Report Generation | 8,706 multimodal data combinations used to generate QA pairs | [GitHub](https://github.com/keke-nice/MedTVT-R1) |

---

> Last generated: {datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M')} UTC